{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1XBOW8M8zHboa7g3W28o_TRcFGNWmWFhb' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1XBOW8M8zHboa7g3W28o_TRcFGNWmWFhb\" -O made_cv_hw1_data.zip && rm -rf /tmp/cookies.txt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip ./made_cv_hw1_data.zip\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile('made_cv_hw1_data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import tqdm\n",
    "from torch.nn import functional as fnn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "TRAIN_SIZE = 0.8\n",
    "NUM_PTS = 971\n",
    "CROP_SIZE = 128\n",
    "SUBMISSION_HEADER = \"file_name,Point_M0_X,Point_M0_Y,Point_M1_X,Point_M1_Y,Point_M2_X,Point_M2_Y,Point_M3_X,Point_M3_Y,Point_M4_X,Point_M4_Y,Point_M5_X,Point_M5_Y,Point_M6_X,Point_M6_Y,Point_M7_X,Point_M7_Y,Point_M8_X,Point_M8_Y,Point_M9_X,Point_M9_Y,Point_M10_X,Point_M10_Y,Point_M11_X,Point_M11_Y,Point_M12_X,Point_M12_Y,Point_M13_X,Point_M13_Y,Point_M14_X,Point_M14_Y,Point_M15_X,Point_M15_Y,Point_M16_X,Point_M16_Y,Point_M17_X,Point_M17_Y,Point_M18_X,Point_M18_Y,Point_M19_X,Point_M19_Y,Point_M20_X,Point_M20_Y,Point_M21_X,Point_M21_Y,Point_M22_X,Point_M22_Y,Point_M23_X,Point_M23_Y,Point_M24_X,Point_M24_Y,Point_M25_X,Point_M25_Y,Point_M26_X,Point_M26_Y,Point_M27_X,Point_M27_Y,Point_M28_X,Point_M28_Y,Point_M29_X,Point_M29_Y\\n\"\n",
    "\n",
    "\n",
    "class ScaleMinSideToSize(object):\n",
    "    def __init__(self, size=(CROP_SIZE, CROP_SIZE), elem_name='image'):\n",
    "        self.size = torch.tensor(size, dtype=torch.float)\n",
    "        self.elem_name = elem_name\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w, _ = sample[self.elem_name].shape\n",
    "        if h > w:\n",
    "            f = self.size[0] / w\n",
    "        else:\n",
    "            f = self.size[1] / h\n",
    "\n",
    "        sample[self.elem_name] = cv2.resize(sample[self.elem_name], None, fx=f, fy=f, interpolation=cv2.INTER_AREA)\n",
    "        sample[\"scale_coef\"] = f\n",
    "\n",
    "        if 'landmarks' in sample:\n",
    "            landmarks = sample['landmarks'].reshape(-1, 2).float()\n",
    "            landmarks = landmarks * f\n",
    "            sample['landmarks'] = landmarks.reshape(-1)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class CropCenter(object):\n",
    "    def __init__(self, size=128, elem_name='image'):\n",
    "        self.size = size\n",
    "        self.elem_name = elem_name\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample[self.elem_name]\n",
    "        h, w, _ = img.shape\n",
    "        margin_h = (h - self.size) // 2\n",
    "        margin_w = (w - self.size) // 2\n",
    "        sample[self.elem_name] = img[margin_h:margin_h + self.size, margin_w:margin_w + self.size]\n",
    "        sample[\"crop_margin_x\"] = margin_w\n",
    "        sample[\"crop_margin_y\"] = margin_h\n",
    "\n",
    "        if 'landmarks' in sample:\n",
    "            landmarks = sample['landmarks'].reshape(-1, 2)\n",
    "            landmarks -= torch.tensor((margin_w, margin_h), dtype=landmarks.dtype)[None, :]\n",
    "            sample['landmarks'] = landmarks.reshape(-1)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class TransformByKeys(object):\n",
    "    def __init__(self, transform, names):\n",
    "        self.transform = transform\n",
    "        self.names = set(names)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        for name in self.names:\n",
    "            if name in sample:\n",
    "                sample[name] = self.transform(sample[name])\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class ThousandLandmarksDataset(data.Dataset):\n",
    "    def __init__(self, root, transforms, split=\"train\"):\n",
    "        super(ThousandLandmarksDataset, self).__init__()\n",
    "        self.root = root\n",
    "        landmark_file_name = os.path.join(root, 'landmarks.csv') if split is not \"test\" \\\n",
    "            else os.path.join(root, \"test_points.csv\")\n",
    "        images_root = os.path.join(root, \"images\")\n",
    "\n",
    "        self.image_names = []\n",
    "        self.landmarks = []\n",
    "\n",
    "        with open(landmark_file_name, \"rt\") as fp:\n",
    "            num_lines = sum(1 for line in fp)\n",
    "        num_lines -= 1  # header\n",
    "\n",
    "        with open(landmark_file_name, \"rt\") as fp:\n",
    "            i_range=tqdm.tqdm(enumerate(fp))\n",
    "            for i, line in i_range:\n",
    "                if i == 0:\n",
    "                    continue  # skip header\n",
    "                if split == \"train\" and i == int(TRAIN_SIZE * num_lines):\n",
    "                    break  # reached end of train part of data\n",
    "                elif split == \"val\" and i < int(TRAIN_SIZE * num_lines):\n",
    "                    continue  # has not reached start of val part of data\n",
    "                elements = line.strip().split(\"\\t\")\n",
    "                image_name = os.path.join(images_root, elements[0])\n",
    "                self.image_names.append(image_name)\n",
    "\n",
    "                if split in (\"train\", \"val\"):\n",
    "                    landmarks = list(map(np.int16, elements[1:]))\n",
    "                    landmarks = np.array(landmarks, dtype=np.int16).reshape((len(landmarks) // 2, 2))\n",
    "                    self.landmarks.append(landmarks)\n",
    "                i_range.refresh()\n",
    "        i_range.close()\n",
    "        if split in (\"train\", \"val\"):\n",
    "            self.landmarks = torch.as_tensor(self.landmarks)\n",
    "        else:\n",
    "            self.landmarks = None\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        if self.landmarks is not None:\n",
    "            landmarks = self.landmarks[idx]\n",
    "            sample[\"landmarks\"] = landmarks\n",
    "\n",
    "        image = cv2.imread(self.image_names[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        sample[\"image\"] = image\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            sample = self.transforms(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "\n",
    "def restore_landmarks(landmarks, f, margins):\n",
    "    dx, dy = margins\n",
    "    landmarks[:, 0] += dx\n",
    "    landmarks[:, 1] += dy\n",
    "    landmarks /= f\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "def restore_landmarks_batch(landmarks, fs, margins_x, margins_y):\n",
    "    landmarks[:, :, 0] += margins_x[:, None]\n",
    "    landmarks[:, :, 1] += margins_y[:, None]\n",
    "    landmarks /= fs[:, None, None]\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "def create_submission(path_to_data, test_predictions, path_to_submission_file):\n",
    "    test_dir = os.path.join(path_to_data, \"test\")\n",
    "\n",
    "    output_file = path_to_submission_file\n",
    "    wf = open(output_file, 'w')\n",
    "    wf.write(SUBMISSION_HEADER)\n",
    "\n",
    "    mapping_path = os.path.join(test_dir, 'test_points.csv')\n",
    "    mapping = pd.read_csv(mapping_path, delimiter='\\t')\n",
    "\n",
    "    for i, row in mapping.iterrows():\n",
    "        file_name = row[0]\n",
    "        point_index_list = np.array(eval(row[1]))\n",
    "        points_for_image = test_predictions[i]\n",
    "        needed_points = points_for_image[point_index_list].astype(np.int)\n",
    "        wf.write(file_name + ',' + ','.join(map(str, needed_points.reshape(2 * len(point_index_list)))) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    parser = ArgumentParser(__doc__)\n",
    "    parser.add_argument(\"--name\", \"-n\", help=\"Experiment name (for saving checkpoints and submits).\",\n",
    "                        default=\"baseline\")\n",
    "    parser.add_argument(\"--data\", \"-d\", help=\"Path to dir with target images & landmarks.\", default=None)\n",
    "    parser.add_argument(\"--batch-size\", \"-b\", default=512, type=int)  # 512 is OK for resnet18 finetune @ 6Gb of VRAM\n",
    "    parser.add_argument(\"--epochs\", \"-e\", default=1, type=int)\n",
    "    parser.add_argument(\"--learning-rate\", \"-lr\", default=1e-3, type=float)\n",
    "    parser.add_argument(\"--gpu\", action=\"store_true\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def train(model, loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    i_range=tqdm.tqdm(loader, total=len(loader), desc=\"training...\")\n",
    "    for batch in i_range:\n",
    "        images = batch[\"image\"].to(device)  # B x 3 x CROP_SIZE x CROP_SIZE\n",
    "        landmarks = batch[\"landmarks\"]  # B x (2 * NUM_PTS)\n",
    "\n",
    "        pred_landmarks = model(images).cpu()  # B x (2 * NUM_PTS)\n",
    "        loss = loss_fn(pred_landmarks, landmarks) #, reduction=\"mean\"\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        i_range.refresh()\n",
    "    i_range.close()\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "\n",
    "def validate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    i_range=tqdm.tqdm(loader, total=len(loader), desc=\"validation...\")\n",
    "    for batch in i_range:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        landmarks = batch[\"landmarks\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(images).cpu()\n",
    "        loss = loss_fn(pred_landmarks, landmarks) #, reduction=\"mean\"\n",
    "        val_loss.append(loss.item())\n",
    "        i_range.refresh()\n",
    "    i_range.close()\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = np.zeros((len(loader.dataset), NUM_PTS, 2))\n",
    "    i_range=tqdm.tqdm(loader, total=len(loader), desc=\"test prediction...\")\n",
    "    for i, batch in enumerate(i_range):\n",
    "        images = batch[\"image\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(images).cpu()\n",
    "        pred_landmarks = pred_landmarks.numpy().reshape((len(pred_landmarks), NUM_PTS, 2))  # B x NUM_PTS x 2\n",
    "\n",
    "        fs = batch[\"scale_coef\"].numpy()  # B\n",
    "        margins_x = batch[\"crop_margin_x\"].numpy()  # B\n",
    "        margins_y = batch[\"crop_margin_y\"].numpy()  # B\n",
    "        prediction = restore_landmarks_batch(pred_landmarks, fs, margins_x, margins_y)  # B x NUM_PTS x 2\n",
    "        predictions[i * loader.batch_size: (i + 1) * loader.batch_size] = prediction\n",
    "        i_range.refresh()\n",
    "    i_range.close()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "TRAIN_PLOT_KWARGS = {\"c\": \"b\"}\n",
    "TEST_SCATTER_KWARGS = {\"c\": \"y\", \"s\": 100, \"zorder\": 1e10}\n",
    "\n",
    "class InteractivePlot(object):\n",
    "    \"\"\"Plot, that can be updated after creation.\n",
    "\n",
    "    Ensure ```%matplotlib notebook``` header in Jupyter.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, min_x, min_y, max_x, max_y, xlabel=None, ylabel=None, grid=True):\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        if xlabel is not None:\n",
    "            ax.set_xlabel(xlabel)\n",
    "        if ylabel is not None:\n",
    "            ax.set_ylabel(ylabel)\n",
    "        ax.set_xlim(min_x, max_x)\n",
    "        ax.set_ylim(min_y, max_y)\n",
    "        self._fig = fig\n",
    "        self._ax = ax\n",
    "        if grid:\n",
    "            ax.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot(self, xs, ys, **kwargs):\n",
    "        self._ax.plot(xs, ys, **kwargs)\n",
    "        self._fig.canvas.draw()\n",
    "\n",
    "    def scatter(self, xs, ys, **kwargs):\n",
    "        self._ax.scatter(xs, ys, **kwargs)\n",
    "        self._fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    parser = ArgumentParser(__doc__)\n",
    "    parser.add_argument(\"--name\", \"-n\", help=\"Experiment name (for saving checkpoints and submits).\",\n",
    "                        default=\"baseline\")\n",
    "    parser.add_argument(\"--data\", \"-d\", help=\"Path to dir with target images & landmarks.\", default=None)\n",
    "    parser.add_argument(\"--batch-size\", \"-b\", default=512, type=int)  # 512 is OK for resnet18 finetune @ 6Gb of VRAM\n",
    "    parser.add_argument(\"--epochs\", \"-e\", default=1, type=int)\n",
    "    parser.add_argument(\"--learning-rate\", \"-lr\", default=1e-3, type=float)\n",
    "    parser.add_argument(\"--gpu\", action=\"store_true\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def train(model, loader, loss_fn, optimizer,  device, plot = None, plot_kwargs={}, plot_steps=10):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    i_range=tqdm.tqdm(loader, total=len(loader), desc=\"training...\")\n",
    "    i = 0\n",
    "    for batch in i_range:\n",
    "        images = batch[\"image\"].to(device)  # B x 3 x CROP_SIZE x CROP_SIZE\n",
    "        landmarks = batch[\"landmarks\"]  # B x (2 * NUM_PTS)\n",
    "\n",
    "        pred_landmarks = model(images).cpu()  # B x (2 * NUM_PTS)\n",
    "        loss = loss_fn(pred_landmarks, landmarks) #, reduction=\"mean\"\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        i_range.refresh()\n",
    "        \n",
    "        if (plot is not None) and (i > 0) and (i % plot_steps == 0):\n",
    "            # Обновим график.\n",
    "            print(i)\n",
    "            prev_loss_value = np.mean(train_loss[-2 * plot_steps: -plot_steps])\n",
    "            print(prev_loss_value)\n",
    "            cur_loss_value = np.mean(train_loss[-plot_steps:])\n",
    "            print(cur_loss_value)\n",
    "            if prev_loss_value is not None:\n",
    "                plot.plot([i- plot_steps, i], [prev_loss_value, cur_loss_value],\n",
    "                          **plot_kwargs)\n",
    "        i +=1\n",
    "    i_range.close()\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "\n",
    "def validate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    i_range=tqdm.tqdm(loader, total=len(loader), desc=\"validation...\")\n",
    "    for batch in i_range:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        landmarks = batch[\"landmarks\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(images).cpu()\n",
    "        loss = loss_fn(pred_landmarks, landmarks) #, reduction=\"mean\"\n",
    "        val_loss.append(loss.item())\n",
    "        i_range.refresh()\n",
    "    i_range.close()\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = np.zeros((len(loader.dataset), NUM_PTS, 2))\n",
    "    i_range=tqdm.tqdm(loader, total=len(loader), desc=\"test prediction...\")\n",
    "    for i, batch in enumerate(i_range):\n",
    "        images = batch[\"image\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(images).cpu()\n",
    "        pred_landmarks = pred_landmarks.numpy().reshape((len(pred_landmarks), NUM_PTS, 2))  # B x NUM_PTS x 2\n",
    "\n",
    "        fs = batch[\"scale_coef\"].numpy()  # B\n",
    "        margins_x = batch[\"crop_margin_x\"].numpy()  # B\n",
    "        margins_y = batch[\"crop_margin_y\"].numpy()  # B\n",
    "        prediction = restore_landmarks_batch(pred_landmarks, fs, margins_x, margins_y)  # B x NUM_PTS x 2\n",
    "        predictions[i * loader.batch_size: (i + 1) * loader.batch_size] = prediction\n",
    "        i_range.refresh()\n",
    "    i_range.close()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. prepare data & models\n",
    "train_transforms = transforms.Compose([\n",
    "    ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n",
    "    CropCenter(CROP_SIZE),\n",
    "    TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n",
    "    TransformByKeys(transforms.ToTensor(), (\"image\",)),\n",
    "    TransformByKeys(transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), (\"image\",)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "315115it [10:55, 480.48it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Reading data...\")\n",
    "train_dataset = ThousandLandmarksDataset(os.path.join('./data/', 'train'), train_transforms, split=\"train\")\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=256, num_workers=16, pin_memory=True,\n",
    "                                    shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in val_dataloader:\n",
    "    a = i[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(np.transpose(a[0].numpy(), (1, 2, 0)))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(a[0].numpy(), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset[0]['landmarks'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "393931it [02:42, 2418.51it/s]  \n"
     ]
    }
   ],
   "source": [
    "val_dataset = ThousandLandmarksDataset(os.path.join('./data/', 'train'), train_transforms, split=\"val\")\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=256, num_workers=16, pin_memory=True,\n",
    "                                  shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_try = '10_epoch_resnet50_l1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "# torch.log  and math.log is e based\n",
    "class WingLoss(nn.Module):\n",
    "    def __init__(self, omega=10, epsilon=2):\n",
    "        super(WingLoss, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        y = target\n",
    "        y_hat = pred\n",
    "        delta_y = (y - y_hat).abs()\n",
    "        delta_y1 = delta_y[delta_y < self.omega]\n",
    "        delta_y2 = delta_y[delta_y >= self.omega]\n",
    "        loss1 = self.omega * torch.log(1 + delta_y1 / self.epsilon)\n",
    "        C = self.omega - self.omega * math.log(1 + self.omega / self.epsilon)\n",
    "        loss2 = delta_y2 - C\n",
    "        return (loss1.sum() + loss2.sum()) / (len(loss1) + len(loss2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 0.015\n",
    "for i in range(4):\n",
    "    st *= 0.2\n",
    "    print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating model...\")\n",
    "# device = torch.device(\"cuda: 0\") if args.gpu else torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda: 0\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2 * NUM_PTS, bias=True)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)\n",
    "#loss_fn = fnn.mse_loss\n",
    "loss_fn = fnn.smooth_l1_loss\n",
    "#loss_fn = WingLoss()\n",
    "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "#                                               step_size=1,\n",
    "#                                               gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iplot = InteractivePlot(0, 0, len(train_dataloader), 3)\n",
    "%matplotlib notebook\n",
    "\n",
    "best_val_loss = np.inf\n",
    "train_loss = train(model, train_dataloader, loss_fn, optimizer, device=device,plot=iplot, plot_kwargs=TRAIN_PLOT_KWARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = validate(model, val_dataloader, loss_fn, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{name_try}_best.pth\", \"wb\") as fp:\n",
    "    torch.save(model.state_dict(), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   0%|          | 0/1231 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1231/1231 [20:44<00:00,  1.01s/it]\n",
      "validation...: 100%|██████████| 308/308 [01:44<00:00,  2.94it/s]\n",
      "training...:   0%|          | 0/1231 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0:\ttrain loss:   2.6\tval loss:  0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1231/1231 [21:04<00:00,  1.03s/it]\n",
      "validation...: 100%|██████████| 308/308 [01:46<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 1:\ttrain loss:  0.83\tval loss:  0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1231/1231 [21:04<00:00,  1.03s/it]\n",
      "validation...: 100%|██████████| 308/308 [01:46<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 2:\ttrain loss:  0.73\tval loss:  0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1231/1231 [21:04<00:00,  1.03s/it]\n",
      "validation...: 100%|██████████| 308/308 [01:48<00:00,  2.85it/s]\n",
      "training...:   0%|          | 0/1231 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 3:\ttrain loss:  0.68\tval loss:   0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1231/1231 [21:04<00:00,  1.03s/it]\n",
      "validation...: 100%|██████████| 308/308 [01:46<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 4:\ttrain loss:  0.64\tval loss:  0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1231/1231 [21:03<00:00,  1.03s/it]\n",
      "validation...: 100%|██████████| 308/308 [01:44<00:00,  2.95it/s]\n",
      "training...:   0%|          | 0/1231 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 5:\ttrain loss:   0.6\tval loss:  0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1231/1231 [21:03<00:00,  1.03s/it]\n",
      "validation...: 100%|██████████| 308/308 [01:46<00:00,  2.89it/s]\n",
      "training...:   0%|          | 0/1231 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 6:\ttrain loss:  0.58\tval loss:   0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1231/1231 [20:47<00:00,  1.01s/it]\n",
      "validation...: 100%|██████████| 308/308 [01:50<00:00,  2.79it/s]\n",
      "training...:   0%|          | 0/1231 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 7:\ttrain loss:  0.55\tval loss:   0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1231/1231 [21:02<00:00,  1.03s/it]\n",
      "validation...: 100%|██████████| 308/308 [01:46<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 8:\ttrain loss:  0.52\tval loss:  0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1231/1231 [21:03<00:00,  1.03s/it]\n",
      "validation...: 100%|██████████| 308/308 [01:45<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 9:\ttrain loss:   0.5\tval loss:  0.49\n"
     ]
    }
   ],
   "source": [
    "# 2. train & validate\n",
    "print(\"Ready for training...\")\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(10):\n",
    "    train_loss = train(model, train_dataloader, loss_fn, optimizer, device=device)\n",
    "    #lr_scheduler.step()\n",
    "    val_loss = validate(model, val_dataloader, loss_fn, device=device)\n",
    "    print(\"Epoch #{:2}:\\ttrain loss: {:5.2}\\tval loss: {:5.2}\".format(epoch, train_loss, val_loss))\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        with open(f\"{name_try}_best.pth\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2892it [00:01, 2026.64it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "7594it [00:03, 2036.20it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "12550it [00:06, 2059.24it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "19201it [00:09, 2076.99it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "24629it [00:12, 2010.58it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "29501it [00:14, 2022.47it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "34356it [00:16, 1982.04it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "38610it [00:19, 1979.49it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "42759it [00:21, 2041.76it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "46743it [00:23, 2067.65it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "51513it [00:25, 2205.72it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "59169it [00:28, 2043.46it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "65226it [00:31, 2099.09it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "74011it [00:35, 2056.34it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "78202it [00:38, 2050.17it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "86569it [00:42, 2071.11it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "90787it [00:44, 2030.86it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "98924it [00:47, 2190.67it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ThousandLandmarksDataset(os.path.join('./data/', 'test'), train_transforms, split=\"test\")\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=256, num_workers=4, pin_memory=True,\n",
    "                                  shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test prediction...: 100%|██████████| 390/390 [04:27<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{name_try}_best.pth\", \"rb\") as fp:\n",
    "    best_state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "    model.load_state_dict(best_state_dict)\n",
    "\n",
    "test_predictions = predict(model, test_dataloader, device)\n",
    "with open(f\"{name_try}_test_predictions.pkl\", \"wb\") as fp:\n",
    "    pickle.dump({\"image_names\": test_dataset.image_names,\n",
    "                  \"landmarks\": test_predictions}, fp)\n",
    "\n",
    "create_submission('./data/', test_predictions, f\"{name_try}_submit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3. predict\n",
    "test_dataset = ThousandLandmarksDataset(os.path.join('./data/', 'test'), train_transforms, split=\"test\")\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=512, num_workers=4, pin_memory=True,\n",
    "                                  shuffle=False, drop_last=False)\n",
    "\n",
    "with open(f\"{name_try}_best.pth\", \"rb\") as fp:\n",
    "    best_state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "    model.load_state_dict(best_state_dict)\n",
    "\n",
    "test_predictions = predict(model, test_dataloader, device)\n",
    "with open(f\"{name_try}_test_predictions.pkl\", \"wb\") as fp:\n",
    "    pickle.dump({\"image_names\": test_dataset.image_names,\n",
    "                  \"landmarks\": test_predictions}, fp)\n",
    "\n",
    "create_submission('./data/', test_predictions, f\"{name_try}_submit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
